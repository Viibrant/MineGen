{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import cfg\n",
    "from data import build_dataset, make_data_loader\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from tqdm import tqdm\n",
    "from torch.nn import Conv3d, ConstantPad3d\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import random"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting categories\n",
    "\n",
    "Using one-hot representation of categories:\n",
    "```python\n",
    "[array(['Arenas', 'Boats', 'Castles', 'Dungeons', 'Entertainement',\n",
    "        'Floating Islands', 'Flying Machines', 'Games', 'Gardens',\n",
    "        'Ground Vehicles', 'Houses And Shops', 'Islands', 'Miscellaneous',\n",
    "        'Pixel Art', 'Redstone', 'Temples', 'Towers', 'Towns', 'Traps'],\n",
    "       dtype=object)]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "          0., 0.]]], dtype=torch.float64)\n",
      "torch.Size([2, 1, 19])\n",
      "torch.Size([2, 1, 128, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def _conv_layer(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv3d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.MaxPool3d(2),\n",
    "        )\n",
    "\n",
    "    \n",
    "    def _linear_layer(self, in_features, out_features):\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(in_features, out_features),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.BatchNorm1d(out_features)\n",
    "        )\n",
    "\n",
    "    def __init__(self):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            self._conv_layer(1, 64, 3, 2, 1),\n",
    "            self._conv_layer(64, 128, 3, 2, 1), # 128, 8, 8, 8\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(65536, 128),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.BatchNorm1d(128),\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(128, 19),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "                    nn.Linear(128, 65536),\n",
    "                    nn.LeakyReLU(inplace=True),\n",
    "                    nn.BatchNorm1d(65536),\n",
    "                    nn.Unflatten(1, (128, 8, 8, 8)),\n",
    "\n",
    "                    nn.ConvTranspose3d(128, 64, kernel_size=3, stride=3, padding= 1),\n",
    "                    nn.LeakyReLU(inplace=True),\n",
    "            \n",
    "                    nn.ConvTranspose3d(64, 32, kernel_size=3, stride=3, padding=1),\n",
    "                    nn.LeakyReLU(inplace=True),\n",
    "\n",
    "                    nn.ConvTranspose3d(32, 1, kernel_size=2, stride=2),\n",
    "                    nn.LeakyReLU(inplace=True),\n",
    "                    nn.Linear(128, 128),\n",
    "                    nn.LeakyReLU(inplace=True),\n",
    "                    nn.BatchNorm3d(1),\n",
    "                \n",
    "                )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        y_hat = self.classifier(encoded)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return y_hat, decoded\n",
    "\n",
    "\n",
    "raw = build_dataset(None)\n",
    "dataset = DataLoader(raw, batch_size=2, shuffle=True)\n",
    "x = make_data_loader(cfg, is_train=True)\n",
    "\n",
    "# print(x.shape)\n",
    "model = AutoEncoder()\n",
    "\n",
    "for schem_data, metadata in dataset:\n",
    "    print(metadata)\n",
    "    print(metadata.shape)\n",
    "    d = schem_data.unsqueeze(1)\n",
    "    output, decoded = model(d)\n",
    "    print(decoded.shape)\n",
    "    # print(decoded)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    model = AutoEncoder()\n",
    "    model.load_state_dict(torch.load('schematic_autoencoder.pth'))\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    model = AutoEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.MSELoss()\n",
    "clf_loss_function = nn.CrossEntropyLoss()\n",
    "optimiser = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "# dataset = DataLoader(build_dataset(None), batch_size=4, shuffle=True)\n",
    "dataset = build_dataset(None)\n",
    "training_data = Subset(dataset, range(0, 2000))\n",
    "training_data = DataLoader(training_data, batch_size=25, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 2175.553288: 100%|██████████| 80/80 [14:22<00:00, 10.79s/it]\n",
      "Epoch 2, Training Loss: 2158.331319: 100%|██████████| 80/80 [14:30<00:00, 10.88s/it]\n",
      "Epoch 3, Training Loss: 2154.913129: 100%|██████████| 80/80 [14:32<00:00, 10.91s/it]\n",
      "Epoch 4, Training Loss: 2151.653074: 100%|██████████| 80/80 [14:33<00:00, 10.92s/it]\n",
      "Epoch 5, Training Loss: 2149.963042: 100%|██████████| 80/80 [14:35<00:00, 10.94s/it]\n",
      "Epoch 6, Training Loss: 2147.374109: 100%|██████████| 80/80 [14:35<00:00, 10.95s/it]\n",
      "Epoch 7, Training Loss: 2145.711355: 100%|██████████| 80/80 [14:36<00:00, 10.96s/it]\n",
      "Epoch 8, Training Loss: 2143.999899: 100%|██████████| 80/80 [14:36<00:00, 10.96s/it]\n",
      "Epoch 9, Training Loss: 2141.524839: 100%|██████████| 80/80 [14:40<00:00, 11.01s/it]\n",
      "Epoch 10, Training Loss: 2140.265988: 100%|██████████| 80/80 [14:38<00:00, 10.98s/it]\n",
      "Epoch 11, Training Loss: 2138.033274: 100%|██████████| 80/80 [14:39<00:00, 10.99s/it]\n",
      "Epoch 12, Training Loss: 2137.113086: 100%|██████████| 80/80 [14:42<00:00, 11.03s/it]\n",
      "Epoch 13, Training Loss: 2136.037801: 100%|██████████| 80/80 [14:39<00:00, 10.99s/it]\n",
      "Epoch 14, Training Loss: 2134.857478: 100%|██████████| 80/80 [14:41<00:00, 11.02s/it]\n",
      "Epoch 15, Training Loss: 2134.102985: 100%|██████████| 80/80 [14:42<00:00, 11.03s/it]\n",
      "Epoch 16, Training Loss: 2132.615350: 100%|██████████| 80/80 [14:45<00:00, 11.06s/it]\n",
      "Epoch 17, Training Loss: 2131.169607: 100%|██████████| 80/80 [14:45<00:00, 11.07s/it]\n",
      "Epoch 18, Training Loss: 2130.758185: 100%|██████████| 80/80 [14:48<00:00, 11.11s/it]\n",
      "Epoch 19, Training Loss: 2129.850078: 100%|██████████| 80/80 [14:47<00:00, 11.10s/it]\n",
      "Epoch 20, Training Loss: 2128.428916: 100%|██████████| 80/80 [14:52<00:00, 11.16s/it]\n",
      "Epoch 21, Training Loss: 2127.787885: 100%|██████████| 80/80 [14:56<00:00, 11.21s/it]\n",
      "Epoch 22, Training Loss: 2126.994248: 100%|██████████| 80/80 [15:00<00:00, 11.26s/it]\n",
      "Epoch 23, Training Loss: 576.370942:  29%|██▉       | 23/80 [04:08<10:16, 10.81s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m optimiser\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     10\u001b[0m schem_data \u001b[39m=\u001b[39m schem_data\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m output, decoded \u001b[39m=\u001b[39m model(schem_data)\n\u001b[1;32m     13\u001b[0m output \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mtype(torch\u001b[39m.\u001b[39mfloat64)\n\u001b[1;32m     15\u001b[0m clf_loss \u001b[39m=\u001b[39m clf_loss_function(output, target\u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/envs/mc/lib/python3.11/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[2], line 57\u001b[0m, in \u001b[0;36mAutoEncoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     55\u001b[0m encoded \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(x)\n\u001b[1;32m     56\u001b[0m y_hat \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclassifier(encoded)\n\u001b[0;32m---> 57\u001b[0m decoded \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder(encoded)\n\u001b[1;32m     58\u001b[0m \u001b[39mreturn\u001b[39;00m y_hat, decoded\n",
      "File \u001b[0;32m~/miniconda3/envs/mc/lib/python3.11/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/mc/lib/python3.11/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/mc/lib/python3.11/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/mc/lib/python3.11/site-packages/torch/nn/modules/conv.py:1108\u001b[0m, in \u001b[0;36mConvTranspose3d.forward\u001b[0;34m(self, input, output_size)\u001b[0m\n\u001b[1;32m   1103\u001b[0m num_spatial_dims \u001b[39m=\u001b[39m \u001b[39m3\u001b[39m\n\u001b[1;32m   1104\u001b[0m output_padding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_padding(\n\u001b[1;32m   1105\u001b[0m     \u001b[39minput\u001b[39m, output_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkernel_size,  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m   1106\u001b[0m     num_spatial_dims, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation)  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m-> 1108\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv_transpose3d(\n\u001b[1;32m   1109\u001b[0m     \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding,\n\u001b[1;32m   1110\u001b[0m     output_padding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training using autoencoder for recreation\n",
    "\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0\n",
    "    for schem_data, target in (pbar := tqdm(training_data)):\n",
    "        optimiser.zero_grad()\n",
    "        schem_data = schem_data.unsqueeze(1)\n",
    "\n",
    "        output, decoded = model(schem_data)\n",
    "        output = output.type(torch.float64)\n",
    "\n",
    "        clf_loss = clf_loss_function(output, target.squeeze(1))\n",
    "        rec_loss = loss_function(decoded, schem_data)\n",
    "\n",
    "        clf_loss.backward(retain_graph=True)\n",
    "        rec_loss.backward()\n",
    "        optimiser.step()\n",
    "        \n",
    "        train_loss += rec_loss.item() + clf_loss.item()\n",
    "        pbar.set_description(f\"Epoch {epoch+1}, Training Loss: {train_loss:.6f}\")\n",
    "    train_loss = train_loss/len(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"schematic_autoencoder.pth\"\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing the model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of our categories are one-hot encoded in this order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Arenas', 'Boats', 'Castles', 'Dungeons', 'Entertainement',\n",
       "       'Floating Islands', 'Flying Machines', 'Games', 'Gardens',\n",
       "       'Ground Vehicles', 'Houses And Shops', 'Islands', 'Miscellaneous',\n",
       "       'Pixel Art', 'Redstone', 'Temples', 'Towers', 'Towns', 'Traps'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CATEGORIES = raw.enc.categories_[0]\n",
    "CATEGORIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predicted_category(output):\n",
    "    return CATEGORIES[output.argmax(dim=1, keepdim=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: Houses And Shops, Actual: Houses And Shops\n",
      "Predicted: Houses And Shops, Actual: Houses And Shops\n",
      "Predicted: Towers, Actual: Towers\n",
      "Predicted: Miscellaneous, Actual: Houses And Shops\n",
      "Predicted: Towers, Actual: Houses And Shops\n",
      "Predicted: Miscellaneous, Actual: Houses And Shops\n",
      "Predicted: Castles, Actual: Castles\n",
      "Predicted: Houses And Shops, Actual: Houses And Shops\n",
      "Predicted: Houses And Shops, Actual: Temples\n",
      "Predicted: Miscellaneous, Actual: Miscellaneous\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 10): \n",
    "    schem_data, target = raw[random.randint(0, len(raw))]\n",
    "    t_ = model.eval()\n",
    "    y_hat, _ = t_.forward(schem_data.unsqueeze(0).unsqueeze(0))\n",
    "    # get predicted class from max value in prediction vector\n",
    "    pred = get_predicted_category(y_hat)\n",
    "    print(f\"Predicted: {pred}, Actual: {CATEGORIES[target.argmax()]}\")\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1, 1]) torch.Size([5, 1, 1])\n",
      "Target category: [[['Towers']]\n",
      "\n",
      " [['Houses And Shops']]\n",
      "\n",
      " [['Houses And Shops']]\n",
      "\n",
      " [['Miscellaneous']]\n",
      "\n",
      " [['Castles']]], Predicted category: [[['Towers']]\n",
      "\n",
      " [['Houses And Shops']]\n",
      "\n",
      " [['Houses And Shops']]\n",
      "\n",
      " [['Miscellaneous']]\n",
      "\n",
      " [['Castles']]]\n",
      "torch.Size([5, 1, 1]) torch.Size([5, 1, 1])\n",
      "Target category: [[['Houses And Shops']]\n",
      "\n",
      " [['Houses And Shops']]\n",
      "\n",
      " [['Houses And Shops']]\n",
      "\n",
      " [['Houses And Shops']]\n",
      "\n",
      " [['Castles']]], Predicted category: [[['Houses And Shops']]\n",
      "\n",
      " [['Houses And Shops']]\n",
      "\n",
      " [['Houses And Shops']]\n",
      "\n",
      " [['Miscellaneous']]\n",
      "\n",
      " [['Castles']]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "unknown is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m testing_data \u001b[39m=\u001b[39m Subset(dataset, \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39m10\u001b[39m))\n\u001b[1;32m     28\u001b[0m testing_data \u001b[39m=\u001b[39m DataLoader(testing_data, batch_size\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> 29\u001b[0m confusion_matrix \u001b[39m=\u001b[39m get_confusion_matrix(model, testing_data)\n",
      "Cell \u001b[0;32mIn[10], line 25\u001b[0m, in \u001b[0;36mget_confusion_matrix\u001b[0;34m(model, dataset)\u001b[0m\n\u001b[1;32m     23\u001b[0m y_true \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(y_true)\n\u001b[1;32m     24\u001b[0m y_pred \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(y_pred)\n\u001b[0;32m---> 25\u001b[0m \u001b[39mreturn\u001b[39;00m confusion_matrix(y_true, y_pred)\n",
      "File \u001b[0;32m~/miniconda3/envs/mc/lib/python3.11/site-packages/sklearn/metrics/_classification.py:317\u001b[0m, in \u001b[0;36mconfusion_matrix\u001b[0;34m(y_true, y_pred, labels, sample_weight, normalize)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconfusion_matrix\u001b[39m(\n\u001b[1;32m    233\u001b[0m     y_true, y_pred, \u001b[39m*\u001b[39m, labels\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, normalize\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m\n\u001b[1;32m    234\u001b[0m ):\n\u001b[1;32m    235\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Compute confusion matrix to evaluate the accuracy of a classification.\u001b[39;00m\n\u001b[1;32m    236\u001b[0m \n\u001b[1;32m    237\u001b[0m \u001b[39m    By definition a confusion matrix :math:`C` is such that :math:`C_{i, j}`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[39m    (0, 2, 1, 1)\u001b[39;00m\n\u001b[1;32m    316\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 317\u001b[0m     y_type, y_true, y_pred \u001b[39m=\u001b[39m _check_targets(y_true, y_pred)\n\u001b[1;32m    318\u001b[0m     \u001b[39mif\u001b[39;00m y_type \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m (\u001b[39m\"\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmulticlass\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    319\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m is not supported\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m y_type)\n",
      "File \u001b[0;32m~/miniconda3/envs/mc/lib/python3.11/site-packages/sklearn/metrics/_classification.py:106\u001b[0m, in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[39m# No metrics support \"multiclass-multioutput\" format\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[39mif\u001b[39;00m y_type \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmulticlass\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmultilabel-indicator\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m--> 106\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m is not supported\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(y_type))\n\u001b[1;32m    108\u001b[0m \u001b[39mif\u001b[39;00m y_type \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmulticlass\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m    109\u001b[0m     y_true \u001b[39m=\u001b[39m column_or_1d(y_true)\n",
      "\u001b[0;31mValueError\u001b[0m: unknown is not supported"
     ]
    }
   ],
   "source": [
    "# confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "def get_confusion_matrix(model, dataset):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for schem_data, target in dataset:\n",
    "        schem_data = schem_data.unsqueeze(1)\n",
    "        output, _ = model(schem_data)\n",
    "\n",
    "        target_idx = target.argmax(dim=2, keepdim=True)\n",
    "        output_idx = output.argmax(dim=1, keepdim=True).unsqueeze(1)\n",
    "\n",
    "        print(target_idx.shape, output_idx.shape)\n",
    "        \n",
    "        print(f\"Target category: {CATEGORIES[target_idx]}, Predicted category: {CATEGORIES[output_idx]}\")\n",
    "\n",
    "        y_true.append(target_idx)\n",
    "        y_pred.append(output_idx)\n",
    "        \n",
    "\n",
    "    y_true = torch.cat(y_true)\n",
    "    y_pred = torch.cat(y_pred)\n",
    "    return confusion_matrix(y_true, y_pred)\n",
    "\n",
    "testing_data = Subset(dataset, range(0, 10))\n",
    "testing_data = DataLoader(testing_data, batch_size=5, shuffle=True)\n",
    "confusion_matrix = get_confusion_matrix(model, testing_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-7.8455e-01,  1.4531e-02, -7.8232e-01, -3.4905e-03, -6.7346e-01,\n",
       "         -4.2006e-01, -4.4595e-01, -6.2003e-01, -2.3628e-01, -6.7820e-01,\n",
       "         -3.0367e-01, -7.3266e-01, -5.2309e-01,  6.7920e-01,  1.7442e-01,\n",
       "         -6.0633e-01, -5.0935e-02, -6.8992e-01, -4.5689e-01, -7.6883e-01,\n",
       "         -5.2528e-01, -4.8645e-01, -5.8308e-01, -3.2763e-01,  2.1635e-01,\n",
       "          2.9517e-01, -3.7397e-01,  2.6488e-01, -1.8199e-01,  8.0500e-01,\n",
       "         -6.4660e-01, -4.2963e-01, -5.0702e-01, -2.4107e-01,  5.4232e-02,\n",
       "         -7.4999e-01,  5.5862e-01, -7.0596e-01,  5.8168e-01, -1.3444e-01,\n",
       "         -1.8748e-01, -1.6629e-01, -3.2080e-01, -6.3765e-01, -4.2578e-01,\n",
       "         -4.5445e-01, -9.0726e-01, -3.6081e-01, -5.9168e-01,  2.4587e-01,\n",
       "         -6.4693e-01, -5.3178e-01, -3.4638e-01,  8.7996e-01,  5.7941e-01,\n",
       "         -4.2231e-01, -1.7770e-01, -5.7971e-01, -6.1181e-01, -1.5868e-01,\n",
       "          5.7187e-01, -3.8504e-01, -8.0563e-02,  6.8467e-01,  1.0760e-01,\n",
       "         -3.3119e-01, -3.7146e-01, -1.7237e-02,  1.0249e+00,  2.8442e-01,\n",
       "         -6.2380e-01, -2.6896e-01, -4.6986e-01, -6.4843e-01, -7.3916e-01,\n",
       "         -5.3562e-01, -3.0716e-02, -7.9728e-01, -5.3005e-01, -4.9373e-01,\n",
       "         -6.5465e-01, -5.4205e-01, -6.5801e-01,  7.5196e-01, -1.0095e-01,\n",
       "          6.5154e-01,  6.5135e-04, -7.4198e-01, -4.4873e-01,  1.1547e-01,\n",
       "         -9.1282e-02, -4.7769e-01, -3.4751e-01, -6.6196e-01, -6.9705e-01,\n",
       "         -4.7005e-01, -3.9050e-01, -4.7438e-02, -5.8722e-01,  4.4038e-01,\n",
       "         -4.0102e-01,  7.9175e-02, -4.1479e-01, -4.3603e-01,  2.1140e-01,\n",
       "         -5.7698e-01, -5.7616e-01, -3.9623e-01, -3.4617e-01,  4.5703e-02,\n",
       "          6.7262e-01, -1.6256e-01, -5.0767e-01, -4.3691e-01, -5.7150e-01,\n",
       "         -8.5407e-02, -5.9844e-02, -4.9528e-01, -1.4396e-01, -2.2768e-01,\n",
       "          5.6551e-01, -3.4161e-01, -4.9796e-02, -4.7268e-01, -1.3167e-01,\n",
       "         -3.3972e-01, -6.1030e-01, -9.1802e-01]],\n",
       "       grad_fn=<NativeBatchNormBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder(schem_data.unsqueeze(0).unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbtschematic import SchematicFile\n",
    "\n",
    "sf = SchematicFile(shape=(128, 128, 128))\n",
    "sf.blocks = np.array(schem_data)\n",
    "sf.save(\"test.schematic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = Subset(build_dataset(None), range(100))\n",
    "sub = DataLoader(sub, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "class VoxelAutoencoder(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(VoxelAutoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv3d(1, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv3d(16, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv3d(32, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv3d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv3d(128, 256, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv3d(256, embedding_dim, kernel_size=3, stride=2, padding=1)\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose3d(embedding_dim, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose3d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose3d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose3d(64, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose3d(32, 16, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose3d(16, 1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        return x_hat, z\n",
    "\n",
    "def train_voxel_embedding(embedding_dim, num_epochs, batch_size, learning_rate, device):\n",
    "    # dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "    dataloader = DataLoader(Subset(dataset, range(20)), batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "    \n",
    "    model = VoxelAutoencoder(embedding_dim).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        for batch in tqdm(sub):\n",
    "            batch = batch.unsqueeze(1).float().to(device) / 255.0\n",
    "            optimizer.zero_grad()\n",
    "            x_hat, z = model(batch)\n",
    "            loss = criterion(x_hat, batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item() * batch.size(0)\n",
    "            \n",
    "        epoch_loss /= len(dpretraining an autataset)\n",
    "        print(f\"Epoch {epoch}: Loss={epoch_loss}\")\n",
    "        \n",
    "    return model\n",
    "\n",
    "train_voxel_embedding(128, 10, 4, 1e-3, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the voxel embedding model\n",
    "class VoxelEmbedding(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Reshape input to be 1D tensor\n",
    "        x = x.view(-1)\n",
    "        # Embed the input\n",
    "        x = self.embedding(x)\n",
    "        # Reshape output to be 3D tensor\n",
    "        x = x.view(-1, 128, 128, 128, -1)\n",
    "        return x\n",
    "\n",
    "# Define the training data\n",
    "train_data = torch.randint(low=0, high=256, size=(1000, 128, 128, 128), dtype=torch.long)\n",
    "\n",
    "# Initialize the model and optimizer\n",
    "model = VoxelEmbedding(num_embeddings=256, embedding_dim=128)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(10):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_data):\n",
    "        optimizer.zero_grad()\n",
    "        inputs = data.unsqueeze(0)\n",
    "        outputs = model(inputs)\n",
    "        loss = outputs.mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} loss: {running_loss / len(train_data)}\")\n",
    "\n",
    "# Test the model\n",
    "test_data = torch.randint(low=0, high=256, size=(1, 128, 128, 128), dtype=torch.long)\n",
    "with torch.no_grad():\n",
    "    inputs = test_data.unsqueeze(0)\n",
    "    outputs = model(inputs)\n",
    "    print(outputs.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a5177f2f352af1d4071d5b0aba21d707598d50ace1793590e952121403ebb087"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
